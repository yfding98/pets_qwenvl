services:
  vllm-qwen_vl:
    container_name: vllm-server-qwen_vl
    image: registry.mthreads.com/mcconline/vllm-musa-qy2-py310:v0.7.3
    privileged: true
    ipc: host
    shm_size: "160gb"
    working_dir: /app
    entrypoint: []
    environment:
      - MTHREADS_VISIBLE_DEVICES=all
    volumes:
      - /data/models:/models
    command:
      - bash
      - -c
      - |        
        vllm serve /models/Qwen2.5-VL-72B-Instruct/ \
        --gpu-memory-utilization 0.8 \
        --served-model-name Qwen2.5-VL-72B-Instruct \
        --max-model-len 8192 \
        --max-num-seqs 64 \
        -tp 8 \
        -pp 1 \
        --trust-remote-code || tail -f /dev/null
    ports:
      - "8020:8000"
    extra_hosts:
      - "host.docker.internal:host-gateway"